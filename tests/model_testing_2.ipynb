{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\.conda\\envs\\UW\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\User\\.conda\\envs\\UW\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "c:\\Users\\User\\.conda\\envs\\UW\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "c:\\Users\\User\\.conda\\envs\\UW\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BART_classifier = pipeline('zero-shot-classification', model='facebook/bart-large-mnli')\n",
    "#mDeBERTa_classifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sectors:11\n",
      "Industry Groups:24\n",
      "Industries:68\n",
      "Sub-Industries:155\n"
     ]
    }
   ],
   "source": [
    "#df=pd.read_csv('india-news-headlines.csv')\n",
    "df2 = pd.read_csv('datasets/US_stocks.csv')\n",
    "\n",
    "df2.drop(['Ticker', 'Company Name', 'Comment'], inplace=True, axis=1)\n",
    "\n",
    "df2_sector = df2.drop(['Industry Group', 'Industry', 'Sub-Industry'], axis=1)\n",
    "df2_industry_group = df2.drop(['Sector', 'Industry', 'Sub-Industry'], axis=1)\n",
    "df2_industry = df2.drop(['Industry Group', 'Sector', 'Sub-Industry'], axis=1)\n",
    "df2_sub_industry = df2.drop(['Industry Group', 'Industry', 'Sector'], axis=1)\n",
    "\n",
    "#df2.head(n=10)\n",
    "#df2_sector.head(n=10)\n",
    "#df2_industry_group.head(n=10)\n",
    "#df2_industry.head(n=10)\n",
    "#df2_sub_industry.head(n=10)\n",
    "#df2_sector.values[0]\n",
    "\n",
    "num_sector=len(df2_sector['Sector'].unique())\n",
    "num_industry_group=len(df2_industry_group['Industry Group'].unique())\n",
    "num_industry=len(df2_industry['Industry'].unique())\n",
    "num_sub_industry=len(df2_sub_industry['Sub-Industry'].unique())\n",
    "\n",
    "print(f'Sectors:{num_sector}\\nIndustry Groups:{num_industry_group}\\nIndustries:{num_industry}\\nSub-Industries:{num_sub_industry}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict Sector with 1st version BART\n",
    "\n",
    "labels = df2_sector['Sector'].unique()\n",
    "text = df2_sector['Description'].head(n=3)\n",
    "\n",
    "hypothesis_template = 'This text is about {}.'\n",
    "predicted_labels=[]\n",
    "for sentence in text:\n",
    "\n",
    "    prediction = BART_classifier(sentence, labels, hypothesis_template=hypothesis_template, multi_label=True)\n",
    "    \n",
    "    predicted_labels.append(prediction['labels'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BartTokenizerFast' object has no attribute 'max_len'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnlp_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m      4\u001b[0m df2_sector[\u001b[39m'\u001b[39m\u001b[39mDescription\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mhead(n\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m lst\u001b[39m=\u001b[39mbart(df2_sector[\u001b[39m'\u001b[39;49m\u001b[39mDescription\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mhead(n\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m), \u001b[39m250\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\User\\OneDrive - Universidade de Aveiro\\Desktop\\UA\\GitReps\\UWEstagio\\tests\\nlp_utils.py:1997\u001b[0m, in \u001b[0;36mbart\u001b[1;34m(corpus, max_len)\u001b[0m\n\u001b[0;32m   1995\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbart\u001b[39m(corpus, max_len):\n\u001b[0;32m   1996\u001b[0m     nlp \u001b[39m=\u001b[39m transformers\u001b[39m.\u001b[39mpipeline(\u001b[39m\"\u001b[39m\u001b[39msummarization\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1997\u001b[0m     lst_summaries \u001b[39m=\u001b[39m [nlp(txt[:nlp\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mmax_len], max_length\u001b[39m=\u001b[39mmax_len\n\u001b[0;32m   1998\u001b[0m                         )[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39msummary_text\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m .\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1999\u001b[0m                      \u001b[39mfor\u001b[39;00m txt \u001b[39min\u001b[39;00m corpus]\n\u001b[0;32m   2000\u001b[0m     \u001b[39mreturn\u001b[39;00m lst_summaries\n",
      "File \u001b[1;32mc:\\Users\\User\\OneDrive - Universidade de Aveiro\\Desktop\\UA\\GitReps\\UWEstagio\\tests\\nlp_utils.py:1997\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1995\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbart\u001b[39m(corpus, max_len):\n\u001b[0;32m   1996\u001b[0m     nlp \u001b[39m=\u001b[39m transformers\u001b[39m.\u001b[39mpipeline(\u001b[39m\"\u001b[39m\u001b[39msummarization\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1997\u001b[0m     lst_summaries \u001b[39m=\u001b[39m [nlp(txt[:nlp\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mmax_len], max_length\u001b[39m=\u001b[39mmax_len\n\u001b[0;32m   1998\u001b[0m                         )[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39msummary_text\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m .\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1999\u001b[0m                      \u001b[39mfor\u001b[39;00m txt \u001b[39min\u001b[39;00m corpus]\n\u001b[0;32m   2000\u001b[0m     \u001b[39mreturn\u001b[39;00m lst_summaries\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BartTokenizerFast' object has no attribute 'max_len'"
     ]
    }
   ],
   "source": [
    "#Text tests\n",
    "from nlp_utils import *\n",
    "\n",
    "df2_sector['Description'].head(n=2)\n",
    "lst=bart(df2_sector['Description'].head(n=2), 250)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text cleanning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict Sector with 2nd version BART\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"AyoubChLin/Bart-MNLI-CNN_news\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"AyoubChLin/Bart-MNLI-CNN_news\")\n",
    "classifier = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Industrials', 'Industrials', 'Industrials']\n",
      "                                         Description            Sector\n",
      "0  Corteva, Inc. is a global provider of seed and...         Materials\n",
      "1  Alico, Inc. is an agribusiness and land manage...  Consumer Staples\n",
      "2  Limoneira Company is primarily an agribusiness...  Consumer Staples\n"
     ]
    }
   ],
   "source": [
    "print(predicted_labels)\n",
    "print(df2_sector.head(n=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
